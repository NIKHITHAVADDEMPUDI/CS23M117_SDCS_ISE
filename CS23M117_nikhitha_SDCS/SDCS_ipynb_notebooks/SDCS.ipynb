{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SOFTWARE DOCUMENTATION USING CODE SUMMARISATION**"
      ],
      "metadata": {
        "id": "ZevFKNSZlWpD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz3WOudldJlp",
        "outputId": "d5e69ac4-068c-4f8f-dd6a-f1fed310406f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCNHt14tdRcm",
        "outputId": "acfb3f95-4429-478c-84ba-c2c09b5f577f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import os\n",
        "\n",
        "# Input folder containing Python source files\n",
        "fine_tuned_model_path =input(\"Enter the path of the fine-tuned model: \")\n",
        "input_folder = input(\"Enter the path of the folder containing Python source files: \")\n",
        "\n",
        "# Load fine-tuned BART tokenizer and model\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "fine_tuned_model = BartForConditionalGeneration.from_pretrained(fine_tuned_model_path)\n",
        "\n",
        "# Load BART-large-cnn tokenizer and model\n",
        "bart_large_cnn_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_large_cnn_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "def preprocess_folder(folder_path):\n",
        "    source_files = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".py\"):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                source_files.append(file.read())\n",
        "    return source_files\n",
        "\n",
        "# Step 2: Method Summarization using fine-tuned BART model\n",
        "def summarize_method(method_text, tokenizer, model):\n",
        "    input_ids = tokenizer.encode(\"summarize: \" + method_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(input_ids, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Step 3: Source File Summarization using BART-large-cnn model\n",
        "def summarize_source_file(method_summaries, tokenizer, model):\n",
        "    input_text = \" \".join(method_summaries)\n",
        "    input_ids = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(input_ids, max_length=300, min_length=100, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "# Step 4: Overall Project Summarization using BART-large-cnn model\n",
        "def summarize_project(source_file_summaries, tokenizer, model):\n",
        "    input_text = \" \".join(source_file_summaries)\n",
        "    input_ids = tokenizer.encode(\"summarize: \" + input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(input_ids, max_length=500, min_length=200, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Step 1: Preprocessing\n",
        "source_files = preprocess_folder(input_folder)\n",
        "\n",
        "# Step 2: Method Summarization using fine-tuned BART model\n",
        "method_summaries = [summarize_method(method, bart_tokenizer, fine_tuned_model) for method in source_files]\n",
        "\n",
        "# Step 3: Source File Summarization using BART-large-cnn model\n",
        "source_file_summary = summarize_source_file(method_summaries, bart_large_cnn_tokenizer, bart_large_cnn_model)\n",
        "\n",
        "# Step 4: Overall Project Summarization using BART-large-cnn model\n",
        "project_summary = summarize_project([source_file_summary], bart_large_cnn_tokenizer, bart_large_cnn_model)\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def simplify_code_summary(summary):\n",
        "    # Split the summary into lines\n",
        "    lines = summary.split(\"\\n\")\n",
        "    updated_lines = []\n",
        "\n",
        "    # Process each line individually\n",
        "    unique_lines = set()\n",
        "    for line in lines:\n",
        "        # Remove consecutive repeating words\n",
        "        line = re.sub(r'\\b(\\w+)(?:\\W+\\1\\b)+', r'\\1', line)\n",
        "\n",
        "        # Remove words identified by dot (.)\n",
        "        line = re.sub(r'\\b\\w+\\.\\w+\\b', '', line)\n",
        "\n",
        "        # Remove specified words from the line\n",
        "        line = re.sub(r'\\b(return|def|summary)\\b', '', line)\n",
        "\n",
        "        # Remove function calls with arguments\n",
        "        line = re.sub(r'\\b\\w+\\s*\\([^()]*\\)\\s*', '', line)\n",
        "\n",
        "        # Introduce new line whenever a period (.) is encountered\n",
        "        line = line.replace('.', '.\\n')\n",
        "\n",
        "        # Remove hashes (#)\n",
        "        line = line.replace('#', '')\n",
        "\n",
        "        # Add line to updated lines if it's not a repetition\n",
        "        if line not in unique_lines:\n",
        "            updated_lines.append(line.strip())\n",
        "            unique_lines.add(line)\n",
        "\n",
        "    # Join the updated lines with new lines\n",
        "    updated_summary = \"\\n\".join(updated_lines)\n",
        "\n",
        "    return updated_summary.strip()\n",
        "\n",
        "print(\"PROJECT SUMMARY\")\n",
        "print(simplify_code_summary(project_summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5yWp1zkdWVo",
        "outputId": "2d648655-8f68-404b-a3a0-51c92c778f04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path of the fine-tuned model: /content/drive/MyDrive/finetuned_model\n",
            "Enter the path of the folder containing Python source files: /content/drive/MyDrive/chess_game\n",
            "PROJECT SUMMARY\n",
            "The game engine scales chess pieces to fit the square size of the board.\n",
            " The game state is displayed as a black and white image of the chess board.\n",
            " It is used to show the state of the game as well as the current state of each piece.\n",
            " It also shows the current position of the player and their position on the board at the start and end of a game.\n",
            " It can also be used to display the position of different pieces at different times in the game, such as when a move is made or a new move is played.\n",
            " For more information on how to play chess in the UK, visit .\n",
            " or go to .\n",
            "com.\n",
            " In the .\n",
            " and Canada, go to  or call the National Chess Association on 1-800-273-8255 or visit a local branch of the NCA.\n",
            " In Europe, the NCCA is based at the University of Edinburgh.\n"
          ]
        }
      ]
    }
  ]
}